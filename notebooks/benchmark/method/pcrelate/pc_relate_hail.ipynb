{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running on Apache Spark version 2.4.4\n",
      "SparkUI available at http://941e42963a7b:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.32-a5876a0a2853\n",
      "LOGGING: writing to /home/rav/repos/gwas-analysis/notebooks/benchmark/method/pcrelate/hail-20200422-1602-0.2.32-a5876a0a2853.log\n"
     ]
    }
   ],
   "source": [
    "import hail as hl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotnine as pn\n",
    "import matplotlib.pyplot as plt\n",
    "hl.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def write_mt(path: Path, n_samples: int, n_variants: int, n_populations: int) -> None:\n",
    "    mt = hl.balding_nichols_model(n_populations=n_populations, n_samples=n_samples, n_variants=n_variants)\n",
    "    mt.write(path.as_posix(), overwrite=True)\n",
    "    \n",
    "def get_mt(n_samples: int, n_variants: int, n_populations: int) -> hl.MatrixTable:\n",
    "    path_mt = Path(f\"/home/rav/data/tmp/mt_{n_samples}_{n_variants}_{n_populations}.mt\")\n",
    "    if not path_mt.exists():\n",
    "        write_mt(path_mt, n_samples, n_variants, n_populations)\n",
    "    return hl.read_matrix_table(path_mt.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Measurment:\n",
    "    samples: int\n",
    "    variants: int\n",
    "    pca_time: float\n",
    "    pc_relate_time: float\n",
    "    \n",
    "    def to_csv(self) -> str:\n",
    "        return f\"{self.samples},{self.variants},{self.pca_time:.2f},{self.pc_relate_time:.2f}\"\n",
    "        \n",
    "measurments = Path(\"/home/rav/data/tmp/measurments\")\n",
    "measurments_dir = measurments.mkdir(parents=True, exist_ok=True)\n",
    "measure_fd = measurments.joinpath(f\"m_{datetime.now().strftime('%d_%m_%Y_%H_%M_%S')}\").open(\"w\")\n",
    "measure_fd.write(\"samples,variants,pca,pc_relate\\n\")\n",
    "measure_fd.flush()\n",
    "\n",
    "m = []\n",
    "\n",
    "def do_experiment(s: int, v: int, p: int) -> None:\n",
    "    mt = get_mt(s, v, p)\n",
    "    # here we assume that there is no recent relatedness in the sample, which should not\n",
    "    # matter for the purpose of the benchmark\n",
    "    start = time.time()\n",
    "    _, scores, _ = hl.methods.hwe_normalized_pca(mt.GT, k=10, compute_loadings=False)\n",
    "    pca_time = time.time() - start\n",
    "    print(f\"PCA took s:{s}, v:{v}, p:{p} : {pca_time:.2f} seconds\")\n",
    "    start = time.time()\n",
    "    pc_relate_result = hl.methods.pc_relate(mt.GT, scores_expr=scores[mt.col_key].scores, min_individual_maf=0.01)\n",
    "    pc_relate_result.write(Path(tempfile.mkdtemp()).joinpath(\"pc_relate_result\").as_posix())\n",
    "    pc_relate_time = time.time() - start\n",
    "    tmp_m = Measurment(samples=s, variants=v, pca_time=pca_time, pc_relate_time=pc_relate_time)\n",
    "    measure_fd.write(f\"{tmp_m.to_csv()}\\n\")\n",
    "    measure_fd.flush()\n",
    "    m.append(tmp_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plink data\n",
    "for n in range(2000, 10001, 2000):\n",
    "    mt = get_mt(n, n, 5)\n",
    "    me = mt.key_cols_by()\n",
    "    # we convert to string and add suffix otherwise having int sample ID can break KING (0)\n",
    "    me = me.transmute_cols(sample_idx=\"S\" + hl.str(me.sample_idx))\n",
    "    me = me.key_cols_by(me.sample_idx)\n",
    "    hl.export_plink(me, f\"/home/rav/data/tmp/plink_{n}/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# square inputs\n",
    "for n in range(5000, 35001, 5000):\n",
    "    do_experiment(s=n, v=n, p=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pure spark pca\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "#from pyspark.mllib.random.RandomRDDs import RandomRDDs\n",
    "from pyspark.mllib.random import RandomRDDs\n",
    "\n",
    "sc = hl.spark_context()\n",
    "mat = RowMatrix(RandomRDDs.uniformVectorRDD(sc, 5000, 5000, numPartitions=200))\n",
    "pc = mat.computePrincipalComponents(10)\n",
    "#projected = mat.multiply(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(5000, 6000, 5000):\n",
    "    for v in range(5000, 6000, 5000):\n",
    "        #for p in range(10, 11, 2):\n",
    "        p = 10\n",
    "        print(f\"Computing s: {s}, v: {v}, p: {p}\")\n",
    "        do_experiment(s=s, v=v, p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
